# NIRMAN Bring ideas to Life 2024 

# SignoSpeak

SignoSpeak is a real-time sign language detection application built using a Convolutional Neural Network (CNN) model. It allows users to communicate using sign language gestures, which are captured through a webcam and translated into text.

## Features

- Real-time sign language detection
- Converts sign language gestures into text
- Supports a wide range of sign language gestures
- Easy-to-use graphical user interface (GUI)

## Requirements

- Python 3.x
- OpenCV
- Keras
- Tkinter
- PIL (Python Imaging Library)

## Installation

1. Clone the SignoSpeak repository:

   ```
   git clone https://github.com/your-username/SignoSpeak.git
   ```

2. Install the required Python packages:

   ```
   pip install opencv-python keras pillow
   ```

## Usage

1. Navigate to the SignoSpeak directory:

   ```
   cd SignoSpeak
   ```

2. Run the SignoSpeak application:

   ```
   python Application.py
   ```

3. The SignoSpeak GUI will open, displaying the webcam feed. Make sign language gestures in front of the webcam, and the application will detect and translate them into text.

## Credits

This application uses a pre-trained CNN model for sign language detection. The model was trained on a dataset of sign language gestures and achieves high accuracy in real-time detection.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- Special thanks to [Author Name] for creating and training the CNN model used in this application.
- This project was inspired by the need to bridge communication gaps between hearing-impaired individuals and the broader community.
