# NIRMAN Bring ideas to Life 2024 

# SignoSpeak: Bridging the Gap

# Problem Statement
Smart Innovation with AI/ML

Mute people use hand signs to communicate, hence normal people face problem in recognizing their language by signs made. Hence there is a need of the systems which recognizes the different signs and conveys the information to the normal people

# Solution

The objective of this project is to develop a robust system capable of accurately recognizing and converting American Sign Language (ASL) gestures representing the 26 alphabets into text. Leveraging Convolutional Neural Networks (CNN) implemented using TensorFlow and OpenCV, the system will analyze input images or video streams captured in real-time through cameras. The CNN model will be trained on a dataset containing a comprehensive collection of ASL alphabet gestures to learn and recognize the unique patterns associated with each sign. The system will then translate the recognized gestures into corresponding textual representations, facilitating communication for individuals with hearing impairments. The key challenges include designing an efficient CNN architecture, preprocessing input images to enhance feature extraction, and optimizing the model's accuracy and performance. By providing a reliable ASL-to-text conversion tool, this project aims to enhance accessibility and promote effective communication for the deaf and hard of hearing community.
